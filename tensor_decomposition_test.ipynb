{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shadialameddin/tensor_decomposition/blob/master/tensor_decomposition_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_quEpBUTTvg"
   },
   "source": [
    "http://tensorly.org/stable/auto_examples/index.html#general-examples\n",
    "\n",
    "https://github.com/tensorly/tensorly\n",
    "\n",
    "\n",
    "Sparse backend\n",
    "\n",
    "Kruskal-tensors have been renamed cp_tensors\n",
    "\n",
    "Matrix-product-state has now been renamed tensor-train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F3DAy_TUQoSc",
    "outputId": "8f864662-289d-4a48-c253-342a4d5df0a4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install -U tensorly --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('tensor_decomposition') and not os.path.isfile('TensorDecomp.py'):\n",
    "    !git clone https://github.com/shadialameddin/tensor_decomposition.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd tensor_decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TensorDecomp import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list of available decomposition types:\n",
    "\n",
    "decomp_list = ['svd', 'parafac', 'tucker', 'matrix_product_state', 'NMF','non_negative_parafac', 'clarkson_woodruff_transform']\n",
    "#### The Error Handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_fro  = lambda  x, y : norm(x-y, ord='fro') / norm(x, ord='fro')\n",
    "err_L1   = lambda  x, y : norm(x-y, ord=1) / norm(x, ord=1)\n",
    "err_Linf = lambda  x, y : norm(x-y, ord=inf) / norm(x, ord=inf)\n",
    "err_Spec = lambda  x, y : norm(x-y, ord=2) / norm(x, ord=2)\n",
    "    \n",
    "normL = [err_fro, err_L1, err_Linf, err_Spec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngp, d, N = 8192, 6, 64\n",
    "\n",
    "# the tensor of interest \n",
    "tensor = np.random.rand(ngp, d, N)\n",
    "\n",
    "# two derived matrices of the tensor (unfolding)\n",
    "mat_repr1_tens = tensor.reshape(-1, N)\n",
    "mat_repr2_tens = tensor.reshape(ngp,-1)\n",
    "\n",
    "# a vector with appropriate dimension to perform operations\n",
    "vec = np.random.rand(N)\n",
    "\n",
    "# a matrix with appropriate dimensions to perform operations\n",
    "someMatrix = np.random.rand(ngp, d)\n",
    "\n",
    "# a tensor with appropriate dimensions to perform operations\n",
    "someTensor = np.random.rand(ngp, d, d)\n",
    "\n",
    "### Some Tensor Operations ###\n",
    "\n",
    "# tensor x some vector\n",
    "tensVec = tensor@vec\n",
    "\n",
    "# tensor x some matrix\n",
    "tensMat = np.einsum('ilk,il->k', tensor, someMatrix, optimize='optimal')\n",
    "\n",
    "# tensor x some tensor x tensor\n",
    "tensTenstens = np.einsum('ilk,ilm,imp->kp', tensor, someTensor, tensor, optimize='optimal')\n",
    "\n",
    "# matrix representation of a tensor x some vector\n",
    "matRepr1Vec = mat_repr1_tens@vec\n",
    "\n",
    "# matrix representation of tensor x some matrix\n",
    "matRepr1Mat = mat_repr1_tens.T@someMatrix.flatten()\n",
    "\n",
    "# matrix representations of a tensor x some tensor x matrix representations of a tensor\n",
    "matReprTensMatRepr = mat_repr1_tens.T @ blk_diag(someTensor) @ mat_repr1_tens\n",
    "\n",
    "### TIME MEASUREMENT OF TENSOR OPERATIONS ###\n",
    "n = 1000\n",
    "\n",
    "# time of tensor x some vector\n",
    "time_tensVec = timeit.timeit( lambda: tensor@vec, number = n)/n\n",
    "\n",
    "# time of tensor x some matrix\n",
    "time_tensMat = timeit.timeit(lambda:np.einsum('ilk,il->k', tensor, someMatrix, optimize='optimal'), number = n)/n\n",
    "\n",
    "# time of tensor x some other tensor x tensor\n",
    "time_tensTenstens = timeit.timeit( lambda:np.einsum('ilk,ilm,imp->kp', tensor, someTensor, tensor, optimize='optimal'), number = n)/n\n",
    "\n",
    "# time of matrix representation of a tensor x some vector\n",
    "time_matRepr1Vec = timeit.timeit( lambda: mat_repr1_tens@vec, number = n)/n\n",
    "\n",
    "# time of matrix representations of a tensor x some tensor x matrix representations of a tensor\n",
    "bsr = blk_diag(someTensor)\n",
    "time_matReprTensMatRepr = timeit.timeit( lambda:mat_repr1_tens.T @ bsr @ mat_repr1_tens, number = n)/n\n",
    "\n",
    "# time of matrix representation of tensor x some matrix\n",
    "flattedMat = someMatrix.flatten()\n",
    "time_matRepr1Mat = timeit.timeit( lambda: mat_repr1_tens.T@flattedMat, number = n)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49152, 4) (4,) (4, 64)\n"
     ]
    }
   ],
   "source": [
    "svd_rank = 4\n",
    "u, s, vt = svds(mat_repr1_tens, svd_rank)\n",
    "print(u.shape,s.shape,vt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SVD Decomposition Time:\t 0.4087625299999999\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "svdDecTime = timeit.timeit((lambda: svds(mat_repr1_tens, svd_rank)), number=n)/n\n",
    "print(\"Average SVD Decomposition Time:\\t\", svdDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with SVD using 4 Rank:\t 0.9374173482259115\n"
     ]
    }
   ],
   "source": [
    "memSavSvd = (tensor.nbytes - sum(f.nbytes for f in [u,s,vt])) / tensor.nbytes\n",
    "print(f\"Memory saving with SVD using {svd_rank} Rank:\\t\", memSavSvd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of the SVD Decomposition Error:\t 0.48329709001310717\n",
      "L1 Norm of the SVD Decomposition Error:\t\t 0.490771157386536\n",
      "L infinity Norm of the SVD Decomposition Error:\t 0.49834885688843267\n",
      "Spectral Norm of the SVD Decomposition Error:\t 0.07403597975272733\n"
     ]
    }
   ],
   "source": [
    "mat_repr1_recons = u @ np.diag(s) @ vt\n",
    "svdFro, svdL1, svdLinf, svdSpec = [error(mat_repr1_tens, mat_repr1_recons) for error in normL ]\n",
    "\n",
    "print(\"Frobenius Norm of the SVD Decomposition Error:\\t\", svdFro)\n",
    "print(\"L1 Norm of the SVD Decomposition Error:\\t\\t\", svdL1)\n",
    "print(\"L infinity Norm of the SVD Decomposition Error:\\t\", svdLinf)\n",
    "print(\"Spectral Norm of the SVD Decomposition Error:\\t\", svdSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error in Decomposed Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03851930808924546 0.031167312469410124 0.06977559372226519 0.01603640512917265\n",
      "0.0014154142468303427\n",
      "0.0067630563394000765 0.0030534677716589327 0.0033149433269609157 0.0011573581665948944\n"
     ]
    }
   ],
   "source": [
    "# svd of matrix representation of tensor x some vector\n",
    "svdMatReprVec = u @ (np.diag(s) @ (vt @ vec))\n",
    "\n",
    "# svd of matrix representation of tensor x some matrix\n",
    "svdMatReprMat = vt.T @ np.diag(s) @ ( u.T @ someMatrix.flatten())\n",
    "\n",
    "# svd of matrix representation of tensor x some Tensor x svd of matrix representation of tensor\n",
    "svdMatReprTenssvdMatRepr = vt.T @ np.diag(s) @ ( u.T @ bsr @ u) @ np.diag(s) @ vt\n",
    "\n",
    "svdDecVec_fro, svdDecVec_L1, svdDecVec_Linf, svdDecVec_L2 = [norm(tensVec,svdMatReprVec.reshape(tensVec.shape)) for norm in normL]\n",
    "print(svdDecVec_fro, svdDecVec_L1, svdDecVec_Linf, svdDecVec_L2)\n",
    "\n",
    "tensMat_svdMatReprMat_L2 = err_Spec(tensMat,svdMatReprMat)\n",
    "print(tensMat_svdMatReprMat_L2)\n",
    "\n",
    "svdMatReprTenssvdMatRepr_tensTenstens_fro, svdMatReprTenssvdMatRepr_tensTenstens_L1, svdMatReprTenssvdMatRepr_tensTenstens_Linf, svdMatReprTenssvdMatRepr_tensTenstens_L2 = [norm(tensTenstens,svdMatReprTenssvdMatRepr) for norm in normL]\n",
    "print(svdMatReprTenssvdMatRepr_tensTenstens_fro, svdMatReprTenssvdMatRepr_tensTenstens_L1, svdMatReprTenssvdMatRepr_tensTenstens_Linf, svdMatReprTenssvdMatRepr_tensTenstens_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.091784702462747 12.070662884618514 13.178565086011348\n",
      "0.6499206803985514 36.402272632159814 23.658589797146874\n",
      "1.264550800298782 8.413511250465547 10.63931238509901\n"
     ]
    }
   ],
   "source": [
    "# time of svd of matrix representation of tensor x some vector\n",
    "time_svdMatReprVec = timeit.timeit( lambda:u @ (np.diag(s) @ (vt @ vec)), number = n)/n\n",
    "\n",
    "# time of svd of matrix representation of tensor x some matrix\n",
    "time_svdMatReprMat = timeit.timeit( lambda:vt.T @ np.diag(s) @ ( u.T @ flattedMat), number = n)/n\n",
    "\n",
    "# time of svd of matrix representation of tensor x some Tensor x svd of matrix representation of tensor x some\n",
    "time_svdMatReprTenssvdMatRepr = timeit.timeit( lambda:vt.T @ np.diag(s) @ ( u.T @ bsr @ u) @ np.diag(s) @ vt, number = n)/n\n",
    "\n",
    "# Speed up in operations\n",
    "\n",
    "### \n",
    "speedup_tensVec_matReprVec = time_tensVec / time_matRepr1Vec\n",
    "speedup_matRepr1Vec_svdMatReprVec =  time_matRepr1Vec / time_svdMatReprVec\n",
    "speedup_tensVec_svdMatReprVec = time_tensVec / time_svdMatReprVec\n",
    "\n",
    "###\n",
    "speedup_tensMat_matRepr1Mat = time_tensMat / time_matRepr1Mat\n",
    "speedup_matRepr1Mat_svdMatReprMat = time_matRepr1Mat / time_svdMatReprMat\n",
    "speedup_tensMat_svdMatReprMat = time_tensMat / time_svdMatReprMat\n",
    "\n",
    "###\n",
    "speedup_tenstenstens_matReprTensMatRepr = time_tensTenstens / time_matReprTensMatRepr\n",
    "speedup_matReprTensMatRepr_svdMatReprTenssvdMatRepr = time_matReprTensMatRepr / time_svdMatReprTenssvdMatRepr \n",
    "speedup_tenstenstens_svdMatReprTenssvdMatRepr = time_tensTenstens / time_svdMatReprTenssvdMatRepr \n",
    "\n",
    "print(speedup_tensVec_matReprVec, speedup_matRepr1Vec_svdMatReprVec, speedup_tensVec_svdMatReprVec)\n",
    "print(speedup_tensMat_matRepr1Mat, speedup_matRepr1Mat_svdMatReprMat, speedup_tensMat_svdMatReprMat)\n",
    "print(speedup_tenstenstens_matReprTensMatRepr, speedup_matReprTensMatRepr_svdMatReprTenssvdMatRepr, speedup_tenstenstens_svdMatReprTenssvdMatRepr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tucker Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Dimension (8, 4, 2)\n",
      "Factors Dimensions [(8192, 8), (6, 4), (64, 2)]\n"
     ]
    }
   ],
   "source": [
    "rank_tucker = (8,4,2)\n",
    "core, factors = tucker(tensor, rank_tucker)\n",
    "print(\"Core Dimension\",core.shape)\n",
    "print(\"Factors Dimensions\", [f.shape for f in factors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Tucker Decomposition Time:\t 0.38975407000000073\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "tuckerDecTime = timeit.timeit((lambda: tucker(tensor, rank_tucker)), number=n)/n\n",
    "print(\"Average Tucker Decomposition Time:\\t\", tuckerDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with Tucker Decomposition using (8, 4, 2) Rank:\t 0.9790980021158854\n"
     ]
    }
   ],
   "source": [
    "memSavTucker = (tensor.nbytes - (core.nbytes + sum(f.nbytes for f in factors))) / tensor.nbytes\n",
    "print(f\"Memory saving with Tucker Decomposition using {rank_tucker} Rank:\\t\", memSavTucker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of the Tucker Decomposition Error:\t 0.4945658398782035\n",
      "L1 Norm of the Tucker Decomposition Error:\t\t 0.49538288760667526\n",
      "L infinity Norm of the Tucker Decomposition Error:\t 0.5010686951303419\n",
      "Spectral Norm of the Tucker Decomposition Error:\t 0.0743486870163199\n"
     ]
    }
   ],
   "source": [
    "tensor_approx_tucker = np.einsum('lmn,il,jm,kn->ijk',core,factors[0],factors[1],factors[2])\n",
    "tuckerFro, tuckerL1, tuckerLinf, tuckerSpec = [error(tensor.reshape(-1,N), tensor_approx_tucker.reshape(-1,N)) for error in normL ]\n",
    "\n",
    "print(\"Frobenius Norm of the Tucker Decomposition Error:\\t\", tuckerFro)\n",
    "print(\"L1 Norm of the Tucker Decomposition Error:\\t\\t\", tuckerL1)\n",
    "print(\"L infinity Norm of the Tucker Decomposition Error:\\t\", tuckerLinf)\n",
    "print(\"Spectral Norm of the Tucker Decomposition Error:\\t\", tuckerSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposed Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8192, 6, 6)\n",
      "(64, 64)\n"
     ]
    }
   ],
   "source": [
    "print(someTensor.shape)\n",
    "print(tensTenstens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6, 8)\n",
      "(64,)\n",
      "(64,)\n",
      "(64, 64)\n"
     ]
    }
   ],
   "source": [
    "a = np.einsum('ilm,ib-> mlb', someTensor, factors[0])\n",
    "b = np.einsum('bcd, mlb, ec, jd-> j', core, a, factors[1],factors[2] )\n",
    "c = np.einsum('ijk,di,ej,fk->f',core, factors[0], factors[1],factors[2])\n",
    "d = np.einsum('k,l->kl', b,c)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed tensor @ vec\n",
    "tuckerTens_Vec = np.einsum('klm,pk,rl,s->pr', core, factors[0], factors[1], factors[2].T @ vec)\n",
    "\n",
    "# decomposed tensor @ mat\n",
    "tuckerTens_Mat = np.einsum('klm,pk,pl,rm->r', core, (someMatrix.T@factors[0]), factors[1], factors[2])\n",
    "\n",
    "# decomposed tensor @ someTensor @ decomposed tensor\n",
    "# tuckerTens_tensor_tuckerTens = np.einsum( ,optimize='optimal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error between decomposed tensor - matrix and tensor-matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11958530842846052 0.1000583359228446 0.18843760446962168 0.09221516566140522\n",
      "0.001468401870412833\n"
     ]
    }
   ],
   "source": [
    "tuckerTensVec_fro, tuckerTensVec_L1, tuckerTensVec_Linf, tuckerTensVec_L2 = [error(tensVec, tuckerTens_Vec ) for error in normL]\n",
    "\n",
    "print(tuckerTensVec_fro, tuckerTensVec_L1, tuckerTensVec_Linf, tuckerTensVec_L2)\n",
    "\n",
    "tuckerTensMat_L2 = err_Spec(tensMat, tuckerTens_Mat) \n",
    "\n",
    "print(tuckerTensMat_L2)\n",
    "\n",
    "#tuckerTensTens_fro, tuckerTensTens_L1, tuckerTensTens_Linf, tuckerTensTens_L2 = [error(tensTenstens, d) for error in normL]\n",
    "#print(tuckerTensTens_fro, tuckerTensTens_L1, tuckerTensTens_Linf, tuckerTensTens_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "# time of tucker tensor x some vector\n",
    "time_tuckerTens_Vec = timeit.timeit(lambda:np.einsum('klm,pk,rl,s->pr', core, factors[0], factors[1], factors[2].T @ vec, optimize ='optimal'), number = n)/n\n",
    "\n",
    "# time of tucker tensor x some matrix\n",
    "time_tuckerTens_Mat = timeit.timeit( lambda:np.einsum('klm,pk,pl,rm->r', core, (someMatrix.T@factors[0]), factors[1], factors[2],optimize ='optimal'), number = n)/n\n",
    "\n",
    "# time of tucker tensor x some Tensor x tucker tensor\n",
    "#time_parTens_tensor_parTens = timeit.timeit( lambda:vt.T @ np.diag(s) @ ( u.T @ blk_diag(someTensor) @ u) @ np.diag(s) @ vt, number = n)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.503197871691247 2.7011925833534884\n"
     ]
    }
   ],
   "source": [
    "speedUP_tensVec_tuckerTensVec = time_tensVec / time_tuckerTens_Vec\n",
    "speedUP_tensMat_tuckerTensMat = time_tensMat / time_tuckerTens_Mat\n",
    "#speedUP_tensTensortens_parTensTensorParTens = time_tensTenstens / time_time_parTens_tensor_parTens\n",
    "\n",
    "print(speedUP_tensVec_tuckerTensVec,speedUP_tensMat_tuckerTensMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parafac Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parafac weight dimension:\t (4,)\n",
      "Parafac factors dimension:\t [(8192, 4), (6, 4), (64, 4)]\n"
     ]
    }
   ],
   "source": [
    "rank_parafac = 4\n",
    "weight, factors = parafac(tensor, rank_parafac)\n",
    "\n",
    "print(\"Parafac weight dimension:\\t\",weight.shape)\n",
    "print(\"Parafac factors dimension:\\t\", [f.shape for f in factors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Parafac Decomposition Time:\t 9.095884210000001\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "parDecTime = timeit.timeit((lambda: parafac(tensor, rank_parafac)), number=n)/n\n",
    "print(\"Average Parafac Decomposition Time:\\t\", parDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with Parafac using 4 Rank:\t 0.9894930521647135\n"
     ]
    }
   ],
   "source": [
    "parMemSav = (tensor.nbytes - (weight.nbytes + sum(f.nbytes for f in factors))) / tensor.nbytes\n",
    "print(f\"Memory saving with Parafac using {rank_parafac} Rank:\\t\", parMemSav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of the Parafac Decomposition Error:\t 0.49702677240822096\n",
      "L1 Norm of the Parafac Decomposition Error:\t\t 0.4979200951412775\n",
      "L infinity Norm of the Parafac Decomposition Error:\t 0.5022292770809487\n",
      "Spectral Norm of the Parafac Decomposition Error:\t 0.07419106501355106\n"
     ]
    }
   ],
   "source": [
    "tensor_approx_parafac = np.einsum('il,jl,kl->ijk',(factors[0]*weight),factors[1],factors[2])\n",
    "parFro, parL1, parLinf, parSpec = [error(tensor.reshape(-1,N), tensor_approx_parafac.reshape(-1,N)) for error in normL ]\n",
    "\n",
    "print(\"Frobenius Norm of the Parafac Decomposition Error:\\t\", parFro)\n",
    "print(\"L1 Norm of the Parafac Decomposition Error:\\t\\t\", parL1)\n",
    "print(\"L infinity Norm of the Parafac Decomposition Error:\\t\", parLinf)\n",
    "print(\"Spectral Norm of the Parafac Decomposition Error:\\t\", parSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposed Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed tensor @ vec\n",
    "parTens_Vec = np.einsum('il,jl,l->ij',(factors[0]*weight),factors[1],factors[2].T @ vec)\n",
    "\n",
    "# decomposed tensor @ mat\n",
    "parTens_Mat = np.einsum('k,mk,mk,ok->o', weight, someMatrix.T@factors[0], factors[1], factors[2])\n",
    "\n",
    "\n",
    "# decomposed tensor @ someTensor @ decomposed tensor\n",
    "#parTens_tensor_parTens = np.einsum( optimize='optimal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error between decomposed tensor - matrix and tensor-matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0757917819668982 0.06198943038140481 0.12321258054941447 0.03400930798177003\n",
      "0.0014632924891887493\n"
     ]
    }
   ],
   "source": [
    "parTensVec_fro, parTensVec_L1, parTensVec_Linf, parTensVec_L2 = [error(tensVec, parTens_Vec ) for error in normL]\n",
    "\n",
    "print(parTensVec_fro, parTensVec_L1, parTensVec_Linf, parTensVec_L2)\n",
    "\n",
    "parTensMat_L2 = err_Spec(tensMat, parTens_Mat) \n",
    "\n",
    "print(parTensMat_L2)\n",
    "\n",
    "#parTensTens_fro, parTensTens_L1, parTensTens_Linf, parTensTens_L2 = [error(tensTenstens, parTensTens) for error in normL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "\n",
    "# time of Parafac tensor x some vector\n",
    "time_parTens_Vec = timeit.timeit(lambda:np.einsum('il,jl,l->ij',(factors[0]*weight),factors[1],factors[2].T @ vec, optimize ='optimal'), number = n)/n\n",
    "\n",
    "# time of Parafac tensor x some matrix\n",
    "time_parTens_Mat = timeit.timeit( lambda:np.einsum('k,mn,op->o', ((someMatrix.T@factors[0])@weight), factors[1], factors[2],optimize ='optimal'), number = n)/n\n",
    "\n",
    "# time of Parafac tensor x some Tensor x Parafac tensor\n",
    "#time_parTens_tensor_parTens = timeit.timeit( )/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.668955454820509 15.955983192235957\n"
     ]
    }
   ],
   "source": [
    "speedUP_tensVec_parTensVec = time_tensVec / time_parTens_Vec\n",
    "speedUP_tensMat_parTensMat = time_tensMat / time_parTens_Mat\n",
    "#speedUP_tensTensortens_parTensTensorParTens = time_tensTenstens / time_time_parTens_tensor_parTens\n",
    "\n",
    "print(speedUP_tensVec_parTensVec,speedUP_tensMat_parTensMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Train (matrix product state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Train factors dimension:\t [(1, 8192, 4), (4, 6, 4), (4, 64, 1)]\n"
     ]
    }
   ],
   "source": [
    "rank_tt = 4\n",
    "factors = matrix_product_state(tensor, (1,rank_tt,rank_tt,1))\n",
    "print(\"Tensor Train factors dimension:\\t\", [f.shape for f in factors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Tensor Train Decomposition Time:\t 0.05029584600000021\n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "ttDecTime = timeit.timeit((lambda: matrix_product_state(tensor, (1,rank_tt,rank_tt,1))), number=n)/n\n",
    "print(\"Average Tensor Train Decomposition Time:\\t\", ttDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with Tensor Train Decomposition using 4 Rank:\t 0.989471435546875\n"
     ]
    }
   ],
   "source": [
    "memSavTT = (tensor.nbytes - sum(f.nbytes for f in factors)) / tensor.nbytes\n",
    "print(f\"Memory saving with Tensor Train Decomposition using {rank_tt} Rank:\\t\", memSavTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of the Tensor Train Decomposition Error:\t\t 0.4983924063523441\n",
      "L1 Norm of the Tensor Train Decomposition Error:\t\t 0.49857077156162166\n",
      "L infinity Norm of the Tensor Train Decomposition Error:\t 0.501409573187759\n",
      "Spectral Norm of the Tensor Train Decomposition Error:\t\t 0.07450474253399417\n"
     ]
    }
   ],
   "source": [
    "tensor_approx_tt = np.einsum('nil,ljm,mkn->ijk',factors[0],factors[1],factors[2])\n",
    "ttFro, ttL1, ttLinf, ttSpec = [error(tensor.reshape(-1,N), tensor_approx_tt.reshape(-1,N)) for error in normL ]\n",
    "\n",
    "print(\"Frobenius Norm of the Tensor Train Decomposition Error:\\t\\t\", ttFro)\n",
    "print(\"L1 Norm of the Tensor Train Decomposition Error:\\t\\t\", ttL1)\n",
    "print(\"L infinity Norm of the Tensor Train Decomposition Error:\\t\", ttLinf)\n",
    "print(\"Spectral Norm of the Tensor Train Decomposition Error:\\t\\t\", ttSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposed Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed tensor @ vec\n",
    "ttTens_Vec = (factors[0].reshape(-1, factors[0].shape[-1])) @ (factors[1]@(factors[2].reshape(-1,factors[2].shape[1]) @ vec))\n",
    "# decomposed tensor @ mat\n",
    "someMatrix.T@factors[0].reshape(-1,factors[0].shape[-1])\n",
    "ttTens_Mat = np.einsum('bc,cbc,cde->d',someMatrix.T@factors[0].reshape(-1,factors[0].shape[-1]), factors[1], factors[2])\n",
    "\n",
    "# decomposed tensor @ someTensor @ decomposed tensor\n",
    "#tuckerTens_tensor_tuckerTens = np.einsum( optimize='optimal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error between decomposed tensor - matrix and tensor-matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07621554594818372 0.06193824320878679 0.12011733118335412 0.03406591823399025\n",
      "0.001466382235626258\n"
     ]
    }
   ],
   "source": [
    "ttTensVec_fro, ttTensVec_L1, ttTensVec_Linf, ttTensVec_L2 = [error(tensVec, ttTens_Vec ) for error in normL]\n",
    "\n",
    "print(ttTensVec_fro, ttTensVec_L1, ttTensVec_Linf, ttTensVec_L2)\n",
    "\n",
    "ttTensMat_L2 = err_Spec(tensMat,ttTens_Mat)\n",
    "print(ttTensMat_L2)\n",
    "\n",
    "#parTensTens_fro, parTensTens_L1, parTensTens_Linf, parTensTens_L2 = [error(tensTenstens, parTensTens) for error in normL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "\n",
    "# time of tt tensor x some vector\n",
    "time_ttTens_Vec = timeit.timeit(lambda: (factors[0].reshape(-1, factors[0].shape[-1])) @ (factors[1]@(factors[2].reshape(-1,factors[2].shape[1]) @ vec)), number = n)/n\n",
    "\n",
    "# time of tt tensor x some matrix\n",
    "time_ttTens_Mat = timeit.timeit(lambda: np.einsum('bc,cbc,cde->d',someMatrix.T@factors[0].reshape(-1,factors[0].shape[-1]), factors[1], factors[2], optimize = 'optimal'), number = n)/n\n",
    "\n",
    "# time of tt tensor x some Tensor x tucker tensor\n",
    "#time_ttTens_tensor_ttTens = timeit.timeit( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.77837182207703 7.191686845000513\n"
     ]
    }
   ],
   "source": [
    "speedUP_tensVec_ttTensVec = time_tensVec / time_ttTens_Vec\n",
    "speedUP_tensMat_ttTensMat = time_tensMat / time_ttTens_Mat\n",
    "#speedUP_tensTensortens_ttTensTensorParTens = time_tensTenstens / time_ttTens_tensor_ttTens\n",
    "\n",
    "print(speedUP_tensVec_ttTensVec, speedUP_tensMat_ttTensMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non negative parafac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non negative parafac weight dimension:\t (4,)\n",
      "Non negative parafac factors dimension:\t [8192, 6, 64]\n"
     ]
    }
   ],
   "source": [
    "rank_nnp = 4\n",
    "weight, factors = non_negative_parafac(tensor, rank_nnp)\n",
    "\n",
    "print(\"Non negative parafac weight dimension:\\t\",weight.shape)\n",
    "print(\"Non negative parafac factors dimension:\\t\", [len(f) for f in factors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NNP Decomposition Time:\t 9.122743835\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "nnpDecTime = timeit.timeit(lambda: non_negative_parafac(tensor, rank_nnp), number=n)/n\n",
    "print(\"Average NNP Decomposition Time:\\t\", nnpDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with SVD using 4 Rank:\t 0.9894930521647135\n"
     ]
    }
   ],
   "source": [
    "nnpMemSav = (tensor.nbytes - (weight.nbytes + sum(f.nbytes for f in factors))) / tensor.nbytes\n",
    "print(f\"Memory saving with SVD using {rank_nnp} Rank:\\t\", nnpMemSav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of the NNP Decomposition Error:\t 0.4977608178838739\n",
      "L1 Norm of the NNP Decomposition Error:\t\t 0.4973443399521515\n",
      "L infinity Norm of the NNP Decomposition Error:\t 0.497825381566608\n",
      "Spectral Norm of the NNP Decomposition Error:\t 0.07449075191608379\n"
     ]
    }
   ],
   "source": [
    "tensor_approx_nnp = np.einsum('il,jl,kl->ijk',(factors[0]*weight),factors[1],factors[2])\n",
    "nnpFro, nnpL1, nnpLinf, nnpSpec = [error(tensor.reshape(-1,N), tensor_approx_nnp.reshape(-1,N)) for error in normL ]\n",
    "\n",
    "print(\"Frobenius Norm of the NNP Decomposition Error:\\t\", nnpFro)\n",
    "print(\"L1 Norm of the NNP Decomposition Error:\\t\\t\", nnpL1)\n",
    "print(\"L infinity Norm of the NNP Decomposition Error:\\t\", nnpLinf)\n",
    "print(\"Spectral Norm of the NNP Decomposition Error:\\t\", nnpSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposed Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed tensor @ vec\n",
    "nnpTens_Vec = np.einsum('il,jl,l->ij',(factors[0]*weight),factors[1],factors[2].T @ vec)\n",
    "\n",
    "# decomposed tensor @ mat\n",
    "nnpTens_Mat = np.einsum('k,mk,mk,ok->o', weight, someMatrix.T@factors[0], factors[1], factors[2])\n",
    "\n",
    "\n",
    "# decomposed tensor @ someTensor @ decomposed tensor\n",
    "#parTens_tensor_parTens = np.einsum( optimize='optimal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error between decomposed tensor - matrix and tensor-matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06691909284824316 0.05771342798850456 0.11362638576491105 0.0339751657685027\n",
      "0.0014883386245672049\n"
     ]
    }
   ],
   "source": [
    "nnpTensVec_fro, nnpTensVec_L1, nnpTensVec_Linf, nnpTensVec_L2 = [error(tensVec, nnpTens_Vec ) for error in normL]\n",
    "\n",
    "print(nnpTensVec_fro, nnpTensVec_L1, nnpTensVec_Linf, nnpTensVec_L2)\n",
    "\n",
    "nnpTensMat_L2 = err_Spec(tensMat, nnpTens_Mat) \n",
    "\n",
    "print(nnpTensMat_L2)\n",
    "\n",
    "#nnpTensTens_fro, nnpTensTens_L1, nnpTensTens_Linf, nnpTensTens_L2 = [error(tensTenstens, nnpTensTens) for error in normL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "\n",
    "# time of NNP tensor x some vector\n",
    "time_nnpTens_Vec = timeit.timeit(lambda:np.einsum('il,jl,l->ij',(factors[0]*weight),factors[1],factors[2].T @ vec, optimize ='optimal'), number = n)/n\n",
    "\n",
    "# time of NNP tensor x some matrix\n",
    "time_nnpTens_Mat = timeit.timeit(lambda:np.einsum('k,mk,mk,ok->o', weight, someMatrix.T@factors[0], factors[1], factors[2], optimize = 'optimal'), number = n)/n\n",
    "\n",
    "# time of NNP tensor x some Tensor x NNP tensor\n",
    "#time_nnpTens_tensor_nnpTens = timeit.timeit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.694849408290377 9.96306827439374\n"
     ]
    }
   ],
   "source": [
    "speedUP_tensVec_nnpTensVec = time_tensVec / time_nnpTens_Vec\n",
    "speedUP_tensMat_nnpTensMat = time_tensMat / time_nnpTens_Mat\n",
    "#speedUP_tensTensortens_parTensTensorParTens = time_tensTenstens / time_time_parTens_tensor_parTens\n",
    "\n",
    "print(speedUP_tensVec_nnpTensVec,speedUP_tensMat_nnpTensMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clarkson Woodruff Transform (Sketching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg._sketches import cwt_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of the matrix representation of the tensor:\t (49152, 64)\n",
      "The CWT of the matrix representation of the tensor dimension:\t (4, 64)\n"
     ]
    }
   ],
   "source": [
    "print('The dimension of the matrix representation of the tensor:\\t',mat_repr1_tens.shape)\n",
    "sketch_n_rows = 4\n",
    "sketch_mat_repr1_tens = clarkson_woodruff_transform(mat_repr1_tens,sketch_n_rows)\n",
    "sketch_someMatrix = clarkson_woodruff_transform(someMatrix.flatten(),sketch_n_rows)\n",
    "mat_repr1_tens_someMatrix = clarkson_woodruff_transform(np.hstack((someMatrix.reshape(-1,1),mat_repr1_tens)),sketch_n_rows)\n",
    "print('The CWT of the matrix representation of the tensor dimension:\\t', sketch_mat_repr1_tens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sketching Time:\t 0.0038552770000001148\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "sketchDecTime = timeit.timeit((lambda: clarkson_woodruff_transform(mat_repr1_tens,sketch_n_rows)), number=n)/n\n",
    "print(\"Average Sketching Time:\\t\", sketchDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with Sketching using 4 rows:\t 0.9999186197916666\n"
     ]
    }
   ],
   "source": [
    "sketchMemSav = (mat_repr1_tens.nbytes - sketch_mat_repr1_tens.nbytes) / mat_repr1_tens.nbytes\n",
    "print(f\"Memory saving with Sketching using {sketch_n_rows} rows:\\t\", sketchMemSav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error between CWT matrix - vector and Matrix - vector operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of matrix representation of the tensor times a vector:\t (49152,)\n",
      "The Error between CWT of the matrix representation of the tensor times a vector and SVD of the matrix representation of the tensor:\n",
      " 0.6622002860135213\n"
     ]
    }
   ],
   "source": [
    "cwtMatvec_L2 = np.abs(norm(svdMatReprVec)-norm(sketch_mat_repr1_tens@vec))/norm(matRepr1Vec)\n",
    "\n",
    "\n",
    "print('The dimension of matrix representation of the tensor times a vector:\\t', matRepr1Vec.shape)\n",
    "print('The Error between CWT of the matrix representation of the tensor times a vector and SVD of the matrix representation of the tensor:\\n', cwtMatvec_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error between tensor times matrix and the CWT of matrix representation of a tensor times cwt of a matrix:\n",
      " 0.8253476254734611\n"
     ]
    }
   ],
   "source": [
    "cwt_mat_repr1_tens_someMatrix = clarkson_woodruff_transform(np.hstack((someMatrix.reshape(-1,1),mat_repr1_tens)),sketch_n_rows)\n",
    "mat_repr1_tens_someMatrix = cwt_mat_repr1_tens_someMatrix[:,1:].T @ cwt_mat_repr1_tens_someMatrix[:,0]\n",
    "# The error between tensor times matrix and the CWT of matrix representation of a tensor times cwt of a matrix\n",
    "cwt_matRepr1_someMatrix = err_Spec(tensMat, mat_repr1_tens_someMatrix )\n",
    "print('The error between tensor times matrix and the CWT of matrix representation of a tensor times cwt of a matrix:\\n',cwt_matRepr1_someMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of W:\t (49152, 4)\n",
      "Dimension of H:\t (4, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_rank = 4\n",
    "model = NMF(nmf_rank, init= 'random', max_iter = 5000)\n",
    "W = model.fit_transform(mat_repr1_tens)\n",
    "H = model.components_\n",
    "\n",
    "print('Dimension of W:\\t',W.shape)\n",
    "print('Dimension of H:\\t',H.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeNMF(model):\n",
    "    model.fit_transform(mat_repr1_tens)\n",
    "    model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NMF Decomposition Time:\t 46.90699879999988\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "nmfDecTime = timeit.timeit(lambda: timeNMF(model), number=n)/n\n",
    "print(\"Average NMF Decomposition Time:\\t\", nmfDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with NMF using 4 Rank:\t 0.9374186197916666\n"
     ]
    }
   ],
   "source": [
    "memSavNMF = (tensor.nbytes - sum([W.nbytes, H.nbytes])) / tensor.nbytes\n",
    "print(f\"Memory saving with NMF using {nmf_rank} Rank:\\t\", memSavNMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of the SVD Decomposition Error:\t 0.4833135559464522\n",
      "L1 Norm of the SVD Decomposition Error:\t\t 0.49079781090580965\n",
      "L infinity Norm of the SVD Decomposition Error:\t 0.4940666882770162\n",
      "Spectral Norm of the SVD Decomposition Error:\t 0.07404253689087155\n"
     ]
    }
   ],
   "source": [
    "mat_repr1_recons_NMF = W@H\n",
    "nmfFro, nmfL1, nmfLinf, nmfSpec = [error(mat_repr1_tens, mat_repr1_recons_NMF) for error in normL ]\n",
    "\n",
    "print(\"Frobenius Norm of the SVD Decomposition Error:\\t\", nmfFro)\n",
    "print(\"L1 Norm of the SVD Decomposition Error:\\t\\t\", nmfL1)\n",
    "print(\"L infinity Norm of the SVD Decomposition Error:\\t\", nmfLinf)\n",
    "print(\"Spectral Norm of the SVD Decomposition Error:\\t\", nmfSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error in decomposed matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03845741652548242 0.03108160518215886 0.0686644459311391 0.01601354184496098\n",
      "0.0014276542379442231\n",
      "0.0067639393397485705 0.0030852004348520755 0.003340428111220827 0.001141463289613436\n"
     ]
    }
   ],
   "source": [
    "# nmf of matrix representation of tensor x some vector\n",
    "nmfMatReprVec = W@(H@vec)\n",
    "\n",
    "# nmf of matrix representation of tensor x some matrix\n",
    "nmfMatReprMat = (someMatrix.flatten().T @ W)@H\n",
    "\n",
    "# nmf of matrix representation of tensor x some Tensor x nmf of matrix representation of tensor \n",
    "nmfMatReprTensNmfMatRepr = H.T @ ( W.T @ bsr @ W) @ H\n",
    "\n",
    "nmfDecVec_fro, nmfDecVec_L1, nmfDecVec_Linf, nmfDecVec_L2 = [norm(tensVec,nmfMatReprVec.reshape(tensVec.shape)) for norm in normL]\n",
    "print(nmfDecVec_fro, nmfDecVec_L1, nmfDecVec_Linf, nmfDecVec_L2)\n",
    "\n",
    "tensMat_nmfMatReprMat_L2 = err_Spec(tensMat,nmfMatReprMat)\n",
    "print(tensMat_nmfMatReprMat_L2)\n",
    "\n",
    "nmfMatReprTensNmfMatRepr_tensTenstens_fro, nmfMatReprTensNmfMatRepr_tensTenstens_L1, nmfMatReprTensNmfMatRepr_tensTenstens_Linf, nmfMatReprTensNmfMatRepr_tensTenstens_L2 = [norm(tensTenstens,nmfMatReprTensNmfMatRepr) for norm in normL]\n",
    "print(nmfMatReprTensNmfMatRepr_tensTenstens_fro, nmfMatReprTensNmfMatRepr_tensTenstens_L1, nmfMatReprTensNmfMatRepr_tensTenstens_Linf, nmfMatReprTensNmfMatRepr_tensTenstens_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.091784702462747 6.330851872239446 6.911927227668668\n",
      "0.6499206803985514 12.65349345779498 8.223767077508732\n",
      "1.264550800298782 6.910558501292934 8.738752283321531\n"
     ]
    }
   ],
   "source": [
    "# time of nmf of matrix representation of tensor x some vector\n",
    "time_nmfMatReprVec = timeit.timeit( lambda: W@(H@vec), number = n)/n\n",
    "\n",
    "# time of nmf of matrix representation of tensor x some matrix\n",
    "time_nmfMatReprMat = timeit.timeit(lambda: flattedMat.T @ W@H, number = n)/n\n",
    "\n",
    "# time of nmf of matrix representation of tensor x some Tensor x nmf of matrix representation of tensor\n",
    "time_nmfMatReprTensNmfMatRepr = timeit.timeit( lambda:H.T @ ( W.T @ bsr @ W) @ H, number = n)/n\n",
    "\n",
    "# Speed up in operations\n",
    "\n",
    "### \n",
    "speedup_tensVec_matReprVec = time_tensVec / time_matRepr1Vec\n",
    "speedup_matRepr1Vec_nmfMatReprVec =  time_matRepr1Vec / time_nmfMatReprVec\n",
    "speedup_tensVec_nmfMatReprVec = time_tensVec / time_nmfMatReprVec\n",
    "\n",
    "###\n",
    "speedup_tensMat_matRepr1Mat = time_tensMat / time_matRepr1Mat\n",
    "speedup_matRepr1Mat_nmfMatReprMat = time_matRepr1Mat / time_nmfMatReprMat\n",
    "speedup_tensMat_nmfMatReprMat = time_tensMat / time_nmfMatReprMat\n",
    "\n",
    "###\n",
    "speedup_tenstenstens_matReprTensMatRepr = time_tensTenstens / time_matReprTensMatRepr\n",
    "speedup_matReprTensMatRepr_nmfMatReprTensNmfMatRepr = time_matReprTensMatRepr / time_nmfMatReprTensNmfMatRepr \n",
    "speedup_tenstenstens_nmfMatReprTensNmfMatRepr = time_tensTenstens / time_nmfMatReprTensNmfMatRepr \n",
    "\n",
    "print(speedup_tensVec_matReprVec, speedup_matRepr1Vec_nmfMatReprVec, speedup_tensVec_nmfMatReprVec)\n",
    "print(speedup_tensMat_matRepr1Mat, speedup_matRepr1Mat_nmfMatReprMat, speedup_tensMat_nmfMatReprMat)\n",
    "print(speedup_tenstenstens_matReprTensMatRepr, speedup_matReprTensMatRepr_nmfMatReprTensNmfMatRepr, speedup_tenstenstens_nmfMatReprTensNmfMatRepr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Bar plot for error, time, memory saving (among the decomposition types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "xAxis = ['SVD', 'Tucker', 'Parafac', 'Tensor Train', 'Non-negative Parafac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "xAxis = ['SVD', 'Tucker', 'Parafac', 'Tensor Train', 'Non-negative Parafac']\n",
    "y0Axis = [errSvd[0][0], errTucker[0][0], errParafac[0][0], errTT[0][0], errNNP[0][0]]\n",
    "y1Axis = [timeSvd, timeTucker, timeParafac, timeTT, timeNNP]\n",
    "y2Axis = [memSavSvd, memSavTucker, memSavParafac, memSavTT, memSavNNP]\n",
    "colors = [\"crimson\",\"deepskyblue\", \"yellowgreen\"]\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(20,16), sharex=True)\n",
    "for i in range(3):\n",
    "    ax[i].bar(xAxis, eval('y'+str(i)+'Axis'), color=colors[i])\n",
    "    ax[i].yaxis.grid(color='b', alpha=0.5, linestyle='dashed', linewidth=0.5)\n",
    "    ax[i].set_axisbelow(True)\n",
    "    ax[i].set_ylim(0, np.ceil(max(eval('y'+str(i)+'Axis'))) + np.mean(eval('y'+str(i)+'Axis'))) if max(eval('y'+str(i)+'Axis')) > 0 else ax[i].set_ylim(0, np.floor(min(eval('y'+str(i)+'Axis')))-5)\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=16)\n",
    "    for h, v in enumerate(eval('y'+str(i)+'Axis')):\n",
    "        ax[i].text(h-.10, v+.0125, str(np.round(v,4)), color='black', size = 16, weight = \"bold\")\n",
    "\n",
    "plt.xlabel(\"Decomposition Types\", fontsize=24)\n",
    "ax[0].set_ylabel(\"Relative Error [-]\", fontsize=18)\n",
    "ax[1].set_ylabel(\"Time [sec]\", fontsize=18)\n",
    "ax[2].set_ylabel(\"Memory Saving [-]\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operList = [lambda:tensor.tensor@vecR, lambda:vecL@tensor.tensor, lambda:tensor.tensor@matR, lambda:matL@tensor.tensor, lambda:vecL@tensor.tensor@vecR, \n",
    "lambda:matL@tensor.tensor@matR]\n",
    "\n",
    "tensorOperationTimes = tensOpTim(operList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "uniSlightBlue = '#00BEFF'\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20,8))\n",
    "ax.bar(operList, tensorOperationTimes, color = uniSlightBlue)\n",
    "ax.yaxis.grid(color='b', alpha=0.5, linestyle='dashed', linewidth=0.5)\n",
    "plt.xticks(rotation=22.5)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_ylabel(\"Elapsed Time [sec]\", fontsize=24)\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True) \n",
    "formatter.set_powerlimits((-1,1)) \n",
    "ax.yaxis.set_major_formatter(formatter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO #1\n",
    "# Speed comparion is not implemented. No function definition is defined for decomposed_tensor @ Vec multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO #2\n",
    "# For every decomposition type -> Line Plot: decomposition error, speedup time in tensor operations vs decomposed tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE! (11.07.21)\n",
    "# For every decomposition type -> Bar plot: Decomposition error, decomposition time, memory saving"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "tensor_decomposition_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
