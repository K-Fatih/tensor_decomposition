{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shadialameddin/tensor_decomposition/blob/master/tensor_decomposition_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_quEpBUTTvg"
   },
   "source": [
    "http://tensorly.org/stable/auto_examples/index.html#general-examples\n",
    "\n",
    "https://github.com/tensorly/tensorly\n",
    "\n",
    "\n",
    "Sparse backend\n",
    "\n",
    "Kruskal-tensors have been renamed cp_tensors\n",
    "\n",
    "Matrix-product-state has now been renamed tensor-train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F3DAy_TUQoSc",
    "outputId": "8f864662-289d-4a48-c253-342a4d5df0a4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U tensorly --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('tensor_decomposition') and not os.path.isfile('TensorDecomp.py'):\n",
    "    !git clone https://github.com/shadialameddin/tensor_decomposition.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd tensor_decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TensorDecomp import *\n",
    "import matplotlib.pyplot as plt\n",
    "from read_h5 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list of available decomposition types:\n",
    "\n",
    "decomp_list = ['svd', 'parafac', 'tucker', 'matrix_product_state', 'NMF','non_negative_parafac', 'clarkson_woodruff_transform']\n",
    "#### The Error Handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_fro  = lambda  x, y : norm(x-y, ord='fro') / norm(x, ord='fro')\n",
    "err_L1   = lambda  x, y : norm(x-y, ord=1) / norm(x, ord=1)\n",
    "err_Linf = lambda  x, y : norm(x-y, ord=inf) / norm(x, ord=inf)\n",
    "err_Spec = lambda  x, y : norm(x-y, ord=2) / norm(x, ord=2)\n",
    "    \n",
    "normL = [err_fro, err_L1, err_Linf, err_Spec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor dimension:\t (312, 32, 128)\n"
     ]
    }
   ],
   "source": [
    "# the tensor of interest \n",
    "#number_of_phases, material_index, weights, truncated_strain_modes, singular_values = load_reduced_basis(file_name='data/laminate_sphere.h5', number_of_modes=64)\n",
    "\n",
    "#tensor = np.einsum('...k,k->...k', truncated_strain_modes, singular_values, optimize='optimal')\n",
    "#ngp, d, N = tensor.shape\n",
    "\n",
    "ngp, d, N = 312,32,128\n",
    "tensor = np.random.rand(ngp,d,N)\n",
    "\n",
    "\n",
    "\n",
    "print(\"The tensor dimension:\\t\",tensor.shape)\n",
    "\n",
    "# two derived matrices of the tensor (unfolding)\n",
    "mat_repr1_tens = tensor.reshape(-1, N)\n",
    "mat_repr2_tens = tensor.reshape(ngp,-1)\n",
    "\n",
    "# a vector with appropriate dimension to perform operations\n",
    "vec = np.random.rand(N)\n",
    "\n",
    "# a matrix with appropriate dimensions to perform operations\n",
    "someMatrix = np.random.rand(ngp, d)\n",
    "\n",
    "# a tensor with appropriate dimensions to perform operations\n",
    "someTensor = np.random.rand(ngp, d, d)\n",
    "\n",
    "### Some Tensor Operations ###\n",
    "\n",
    "# tensor x some vector\n",
    "tensVec = tensor@vec\n",
    "\n",
    "# tensor x some matrix\n",
    "tensMat = np.einsum('ilk,il->k', tensor, someMatrix, optimize='optimal')\n",
    "\n",
    "# tensor x some tensor x tensor\n",
    "tensTenstens = np.einsum('ilk,ilm,imp->kp', tensor, someTensor, tensor, optimize='optimal')\n",
    "\n",
    "# matrix representation of a tensor x some vector\n",
    "matRepr1Vec = mat_repr1_tens@vec\n",
    "\n",
    "# matrix representation of tensor x some matrix\n",
    "matRepr1Mat = mat_repr1_tens.T@someMatrix.flatten()\n",
    "\n",
    "# matrix representations of a tensor x some tensor x matrix representations of a tensor\n",
    "matReprTensMatRepr = mat_repr1_tens.T @ blk_diag(someTensor) @ mat_repr1_tens\n",
    "\n",
    "### TIME MEASUREMENT OF TENSOR OPERATIONS ###\n",
    "n = 1000\n",
    "\n",
    "# time of tensor x some vector\n",
    "time_tensVec = timeit.timeit( lambda: tensor@vec, number = n)/n\n",
    "\n",
    "# time of tensor x some matrix\n",
    "time_tensMat = timeit.timeit(lambda:np.einsum('ilk,il->k', tensor, someMatrix, optimize='optimal'), number = n)/n\n",
    "\n",
    "# time of tensor x some other tensor x tensor\n",
    "time_tensTenstens = timeit.timeit( lambda:np.einsum('ilk,ilm,imp->kp', tensor, someTensor, tensor, optimize='optimal'), number = n)/n\n",
    "\n",
    "# time of matrix representation of a tensor x some vector\n",
    "time_matRepr1Vec = timeit.timeit( lambda: mat_repr1_tens@vec, number = n)/n\n",
    "\n",
    "# time of matrix representations of a tensor x some tensor x matrix representations of a tensor\n",
    "bsr = blk_diag(someTensor)\n",
    "time_matReprTensMatRepr = timeit.timeit( lambda:mat_repr1_tens.T @ bsr @ mat_repr1_tens, number = n)/n\n",
    "\n",
    "# time of matrix representation of tensor x some matrix\n",
    "flattedMat = someMatrix.flatten()\n",
    "time_matRepr1Mat = timeit.timeit( lambda: mat_repr1_tens.T@flattedMat, number = n)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "print(tensMat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9984, 4) (4,) (4, 128)\n"
     ]
    }
   ],
   "source": [
    "svd_rank = 4\n",
    "u, s, vt = svds(mat_repr1_tens, svd_rank)\n",
    "print(u.shape,s.shape,vt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SVD Decomposition Time:\t 0.17017640999999983\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "svdDecTime = timeit.timeit((lambda: svds(mat_repr1_tens, svd_rank)), number=n)/n\n",
    "print(\"Average SVD Decomposition Time:\\t\", svdDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with SVD using 4 Rank:\t 0.9683462289663461\n"
     ]
    }
   ],
   "source": [
    "memSavSvd = (tensor.nbytes - sum(f.nbytes for f in [u,s,vt])) / tensor.nbytes\n",
    "print(f\"Memory saving with SVD using {svd_rank} Rank:\\t\", memSavSvd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of the SVD Decomposition Error:\t 0.4908704760775036\n",
      "L1 Norm of the SVD Decomposition Error:\t\t 0.49554955044789906\n",
      "L infinity Norm of the SVD Decomposition Error:\t 0.48515303563920786\n",
      "Spectral Norm of the SVD Decomposition Error:\t 0.05590520801658968\n"
     ]
    }
   ],
   "source": [
    "mat_repr1_recons = u @ np.diag(s) @ vt\n",
    "svdFro, svdL1, svdLinf, svdSpec = [error(mat_repr1_tens, mat_repr1_recons) for error in normL ]\n",
    "\n",
    "print(\"Frobenius Norm of the SVD Decomposition Error:\\t\", svdFro)\n",
    "print(\"L1 Norm of the SVD Decomposition Error:\\t\\t\", svdL1)\n",
    "print(\"L infinity Norm of the SVD Decomposition Error:\\t\", svdLinf)\n",
    "print(\"Spectral Norm of the SVD Decomposition Error:\\t\", svdSpec)\n",
    "\n",
    "svdError = [svdFro, svdL1, svdLinf,svdSpec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error in Decomposed Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028441121950307298 0.024309209881904287 0.03060532866745797 0.006544930166278462\n",
      "0.002891368888803239\n",
      "0.0014295089073983846 0.002163035270008067 0.0018893939941996615 0.0006685303439957029\n"
     ]
    }
   ],
   "source": [
    "# svd of matrix representation of tensor x some vector\n",
    "svdMatReprVec = u @ (np.diag(s) @ (vt @ vec))\n",
    "\n",
    "# svd of matrix representation of tensor x some matrix\n",
    "svdMatReprMat = vt.T @ np.diag(s) @ ( u.T @ someMatrix.flatten())\n",
    "\n",
    "# svd of matrix representation of tensor x some Tensor x svd of matrix representation of tensor\n",
    "svdMatReprTenssvdMatRepr = vt.T @ np.diag(s) @ ( u.T @ bsr @ u) @ np.diag(s) @ vt\n",
    "\n",
    "svdDecVec_fro, svdDecVec_L1, svdDecVec_Linf, svdDecVec_L2 = [norm(tensVec,svdMatReprVec.reshape(tensVec.shape)) for norm in normL]\n",
    "print(svdDecVec_fro, svdDecVec_L1, svdDecVec_Linf, svdDecVec_L2)\n",
    "\n",
    "tensMat_svdMatReprMat_L2 = err_Spec(tensMat,svdMatReprMat)\n",
    "print(tensMat_svdMatReprMat_L2)\n",
    "\n",
    "svdMatReprTenssvdMatRepr_tensTenstens_fro, svdMatReprTenssvdMatRepr_tensTenstens_L1, svdMatReprTenssvdMatRepr_tensTenstens_Linf, svdMatReprTenssvdMatRepr_tensTenstens_L2 = [norm(tensTenstens,svdMatReprTenssvdMatRepr) for norm in normL]\n",
    "print(svdMatReprTenssvdMatRepr_tensTenstens_fro, svdMatReprTenssvdMatRepr_tensTenstens_L1, svdMatReprTenssvdMatRepr_tensTenstens_Linf, svdMatReprTenssvdMatRepr_tensTenstens_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1814049053740645 3.9300926748819056 4.643030764680161\n",
      "1.2453500981083456 26.3459850108154 32.809975017979966\n",
      "1.9019021034218673 13.950418999496614 26.532331238758992\n"
     ]
    }
   ],
   "source": [
    "# time of svd of matrix representation of tensor x some vector\n",
    "time_svdMatReprVec = timeit.timeit( lambda:u @ (np.diag(s) @ (vt @ vec)), number = n)/n\n",
    "\n",
    "# time of svd of matrix representation of tensor x some matrix\n",
    "time_svdMatReprMat = timeit.timeit( lambda:vt.T @ np.diag(s) @ ( u.T @ flattedMat), number = n)/n\n",
    "\n",
    "# time of svd of matrix representation of tensor x some Tensor x svd of matrix representation of tensor x some\n",
    "time_svdMatReprTenssvdMatRepr = timeit.timeit( lambda:vt.T @ np.diag(s) @ ( u.T @ bsr @ u) @ np.diag(s) @ vt, number = n)/n\n",
    "\n",
    "# Speed up in operations\n",
    "\n",
    "### \n",
    "speedup_tensVec_matReprVec = time_tensVec / time_matRepr1Vec\n",
    "speedup_matRepr1Vec_svdMatReprVec =  time_matRepr1Vec / time_svdMatReprVec\n",
    "speedup_tensVec_svdMatReprVec = time_tensVec / time_svdMatReprVec\n",
    "\n",
    "###\n",
    "speedup_tensMat_matRepr1Mat = time_tensMat / time_matRepr1Mat\n",
    "speedup_matRepr1Mat_svdMatReprMat = time_matRepr1Mat / time_svdMatReprMat\n",
    "speedup_tensMat_svdMatReprMat = time_tensMat / time_svdMatReprMat\n",
    "\n",
    "###\n",
    "speedup_tenstenstens_matReprTensMatRepr = time_tensTenstens / time_matReprTensMatRepr\n",
    "speedup_matReprTensMatRepr_svdMatReprTenssvdMatRepr = time_matReprTensMatRepr / time_svdMatReprTenssvdMatRepr \n",
    "speedup_tenstenstens_svdMatReprTenssvdMatRepr = time_tensTenstens / time_svdMatReprTenssvdMatRepr \n",
    "\n",
    "print(speedup_tensVec_matReprVec, speedup_matRepr1Vec_svdMatReprVec, speedup_tensVec_svdMatReprVec)\n",
    "print(speedup_tensMat_matRepr1Mat, speedup_matRepr1Mat_svdMatReprMat, speedup_tensMat_svdMatReprMat)\n",
    "print(speedup_tenstenstens_matReprTensMatRepr, speedup_matReprTensMatRepr_svdMatReprTenssvdMatRepr, speedup_tenstenstens_svdMatReprTenssvdMatRepr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tucker Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Dimension:\t\t (4, 4, 4)\n",
      "Factors Dimensions:\t [(312, 4), (32, 4), (128, 4)]\n"
     ]
    }
   ],
   "source": [
    "rank_tucker = (4,4,4)\n",
    "core, factors = tucker(tensor, rank_tucker)\n",
    "print(\"Core Dimension:\\t\\t\",core.shape)\n",
    "print(\"Factors Dimensions:\\t\", [f.shape for f in factors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Tucker Decomposition Time:\t 0.12264809000000128\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "tuckerDecTime = timeit.timeit((lambda: tucker(tensor, rank_tucker)), number=n)/n\n",
    "print(\"Average Tucker Decomposition Time:\\t\", tuckerDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with Tucker Decomposition using (4, 4, 4) Rank:\t 0.9984725560897436\n"
     ]
    }
   ],
   "source": [
    "memSavTucker = (tensor.nbytes - (core.nbytes + sum(f.nbytes for f in factors))) / tensor.nbytes\n",
    "print(f\"Memory saving with Tucker Decomposition using {rank_tucker} Rank:\\t\", memSavTucker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of the Tucker Decomposition Error:\t 0.4994907659776066\n",
      "L1 Norm of the Tucker Decomposition Error:\t\t 0.500334314480789\n",
      "L infinity Norm of the Tucker Decomposition Error:\t 0.49698834081898857\n",
      "Spectral Norm of the Tucker Decomposition Error:\t 0.055982598344453564\n"
     ]
    }
   ],
   "source": [
    "tensor_approx_tucker = np.einsum('lmn,il,jm,kn->ijk',core,factors[0],factors[1],factors[2])\n",
    "tuckerFro, tuckerL1, tuckerLinf, tuckerSpec = [error(tensor.reshape(-1,N), tensor_approx_tucker.reshape(-1,N)) for error in normL ]\n",
    "\n",
    "print(\"Frobenius Norm of the Tucker Decomposition Error:\\t\", tuckerFro)\n",
    "print(\"L1 Norm of the Tucker Decomposition Error:\\t\\t\", tuckerL1)\n",
    "print(\"L infinity Norm of the Tucker Decomposition Error:\\t\", tuckerLinf)\n",
    "print(\"Spectral Norm of the Tucker Decomposition Error:\\t\", tuckerSpec)\n",
    "\n",
    "tuckerError = [tuckerFro, tuckerL1, tuckerLinf, tuckerSpec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposed Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed tensor @ vec\n",
    "tuckerTens_Vec = np.einsum('klm,pk,rl,s->pr', core, factors[0], factors[1], factors[2].T @ vec)\n",
    "\n",
    "# decomposed tensor @ mat\n",
    "tuckerTens_Mat = np.einsum('klm,pk,pl,rm->r', core, (someMatrix.T@factors[0]), factors[1], factors[2])\n",
    "\n",
    "# decomposed tensor @ someTensor @ decomposed tensor\n",
    "# tuckerTens_tensor_tuckerTens = np.einsum( ,optimize='optimal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error between decomposed tensor - matrix and tensor-matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11316724338802658 0.10510255413041882 0.12415471450769498 0.09461452264561027\n",
      "0.0029197776200218933\n"
     ]
    }
   ],
   "source": [
    "tuckerTensVec_fro, tuckerTensVec_L1, tuckerTensVec_Linf, tuckerTensVec_L2 = [error(tensVec, tuckerTens_Vec ) for error in normL]\n",
    "\n",
    "print(tuckerTensVec_fro, tuckerTensVec_L1, tuckerTensVec_Linf, tuckerTensVec_L2)\n",
    "\n",
    "tuckerTensMat_L2 = err_Spec(tensMat, tuckerTens_Mat) \n",
    "\n",
    "print(tuckerTensMat_L2)\n",
    "\n",
    "#tuckerTensTens_fro, tuckerTensTens_L1, tuckerTensTens_Linf, tuckerTensTens_L2 = [error(tensTenstens, d) for error in normL]\n",
    "#print(tuckerTensTens_fro, tuckerTensTens_L1, tuckerTensTens_Linf, tuckerTensTens_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "# time of tucker tensor x some vector\n",
    "time_tuckerTens_Vec = timeit.timeit(lambda:np.einsum('klm,pk,rl,s->pr', core, factors[0], factors[1], factors[2].T @ vec, optimize ='optimal'), number = n)/n\n",
    "\n",
    "# time of tucker tensor x some matrix\n",
    "time_tuckerTens_Mat = timeit.timeit( lambda:np.einsum('klm,pk,pl,rm->r', core, (someMatrix.T@factors[0]), factors[1], factors[2],optimize ='optimal'), number = n)/n\n",
    "\n",
    "# time of tucker tensor x some Tensor x tucker tensor\n",
    "#time_parTens_tensor_parTens = timeit.timeit( lambda:vt.T @ np.diag(s) @ ( u.T @ blk_diag(someTensor) @ u) @ np.diag(s) @ vt, number = n)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9850174947259291 3.7110382271026654\n"
     ]
    }
   ],
   "source": [
    "speedUP_tensVec_tuckerTensVec = time_tensVec / time_tuckerTens_Vec\n",
    "speedUP_tensMat_tuckerTensMat = time_tensMat / time_tuckerTens_Mat\n",
    "#speedUP_tensTensortens_parTensTensorParTens = time_tensTenstens / time_time_parTens_tensor_parTens\n",
    "\n",
    "print(speedUP_tensVec_tuckerTensVec,speedUP_tensMat_tuckerTensMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parafac Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parafac weight dimension:\t (4,)\n",
      "Parafac factors dimension:\t [(312, 4), (32, 4), (128, 4)]\n"
     ]
    }
   ],
   "source": [
    "rank_parafac = 4\n",
    "weight, factors = parafac(tensor, rank_parafac)\n",
    "\n",
    "print(\"Parafac weight dimension:\\t\",weight.shape)\n",
    "print(\"Parafac factors dimension:\\t\", [f.shape for f in factors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Parafac Decomposition Time:\t 2.9036585399999995\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "parDecTime = timeit.timeit((lambda: parafac(tensor, rank_parafac)), number=n)/n\n",
    "print(\"Average Parafac Decomposition Time:\\t\", parDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with Parafac using 4 Rank:\t 0.9985195062099359\n"
     ]
    }
   ],
   "source": [
    "parMemSav = (tensor.nbytes - (weight.nbytes + sum(f.nbytes for f in factors))) / tensor.nbytes\n",
    "print(f\"Memory saving with Parafac using {rank_parafac} Rank:\\t\", parMemSav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of the Parafac Decomposition Error:\t 0.4993791595583205\n",
      "L1 Norm of the Parafac Decomposition Error:\t\t 0.5004946740455458\n",
      "L infinity Norm of the Parafac Decomposition Error:\t 0.49750126251764826\n",
      "Spectral Norm of the Parafac Decomposition Error:\t 0.05629806201287384\n"
     ]
    }
   ],
   "source": [
    "tensor_approx_parafac = np.einsum('il,jl,kl->ijk',(factors[0]*weight),factors[1],factors[2])\n",
    "parFro, parL1, parLinf, parSpec = [error(tensor.reshape(-1,N), tensor_approx_parafac.reshape(-1,N)) for error in normL ]\n",
    "\n",
    "print(\"Frobenius Norm of the Parafac Decomposition Error:\\t\", parFro)\n",
    "print(\"L1 Norm of the Parafac Decomposition Error:\\t\\t\", parL1)\n",
    "print(\"L infinity Norm of the Parafac Decomposition Error:\\t\", parLinf)\n",
    "print(\"Spectral Norm of the Parafac Decomposition Error:\\t\", parSpec)\n",
    "\n",
    "parafacError = [parFro,parL1,parLinf,parSpec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposed Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed tensor @ vec\n",
    "parTens_Vec = np.einsum('il,jl,l->ij',(factors[0]*weight),factors[1],factors[2].T @ vec)\n",
    "\n",
    "# decomposed tensor @ mat\n",
    "parTens_Mat = np.einsum('k,mk,mk,ok->o', weight, someMatrix.T@factors[0], factors[1], factors[2])\n",
    "\n",
    "\n",
    "# decomposed tensor @ someTensor @ decomposed tensor\n",
    "#parTens_tensor_parTens = np.einsum( optimize='optimal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error between decomposed tensor - matrix and tensor-matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.057318375603369595 0.050054061085266145 0.06344420341481377 0.01357910773866669\n",
      "0.0029199487015274944\n"
     ]
    }
   ],
   "source": [
    "parTensVec_fro, parTensVec_L1, parTensVec_Linf, parTensVec_L2 = [error(tensVec, parTens_Vec ) for error in normL]\n",
    "\n",
    "print(parTensVec_fro, parTensVec_L1, parTensVec_Linf, parTensVec_L2)\n",
    "\n",
    "parTensMat_L2 = err_Spec(tensMat, parTens_Mat) \n",
    "\n",
    "print(parTensMat_L2)\n",
    "\n",
    "#parTensTens_fro, parTensTens_L1, parTensTens_Linf, parTensTens_L2 = [error(tensTenstens, parTensTens) for error in normL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "\n",
    "# time of Parafac tensor x some vector\n",
    "time_parTens_Vec = timeit.timeit(lambda:np.einsum('il,jl,l->ij',(factors[0]*weight),factors[1],factors[2].T @ vec, optimize ='optimal'), number = n)/n\n",
    "\n",
    "# time of Parafac tensor x some matrix\n",
    "time_parTens_Mat = timeit.timeit( lambda:np.einsum('k,mn,op->o', ((someMatrix.T@factors[0])@weight), factors[1], factors[2],optimize ='optimal'), number = n)/n\n",
    "\n",
    "# time of Parafac tensor x some Tensor x Parafac tensor\n",
    "#time_parTens_tensor_parTens = timeit.timeit( )/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.493827498586965 9.611252247734106\n"
     ]
    }
   ],
   "source": [
    "speedUP_tensVec_parTensVec = time_tensVec / time_parTens_Vec\n",
    "speedUP_tensMat_parTensMat = time_tensMat / time_parTens_Mat\n",
    "#speedUP_tensTensortens_parTensTensorParTens = time_tensTenstens / time_time_parTens_tensor_parTens\n",
    "\n",
    "print(speedUP_tensVec_parTensVec,speedUP_tensMat_parTensMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Train (matrix product state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Train factors dimension:\t [(1, 312, 4), (4, 32, 4), (4, 128, 1)]\n"
     ]
    }
   ],
   "source": [
    "rank_tt = 4\n",
    "factors = matrix_product_state(tensor, (1,rank_tt,rank_tt,1))\n",
    "print(\"Tensor Train factors dimension:\\t\", [f.shape for f in factors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Tensor Train Decomposition Time:\t 0.036509006\n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "ttDecTime = timeit.timeit((lambda: matrix_product_state(tensor, (1,rank_tt,rank_tt,1))), number=n)/n\n",
    "print(\"Average Tensor Train Decomposition Time:\\t\", ttDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with Tensor Train Decomposition using 4 Rank:\t 0.998222155448718\n"
     ]
    }
   ],
   "source": [
    "memSavTT = (tensor.nbytes - sum(f.nbytes for f in factors)) / tensor.nbytes\n",
    "print(f\"Memory saving with Tensor Train Decomposition using {rank_tt} Rank:\\t\", memSavTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of the Tensor Train Decomposition Error:\t\t 0.49956643342159157\n",
      "L1 Norm of the Tensor Train Decomposition Error:\t\t 0.5009014068991237\n",
      "L infinity Norm of the Tensor Train Decomposition Error:\t 0.49596789354525367\n",
      "Spectral Norm of the Tensor Train Decomposition Error:\t\t 0.05652133117543093\n"
     ]
    }
   ],
   "source": [
    "tensor_approx_tt = np.einsum('nil,ljm,mkn->ijk',factors[0],factors[1],factors[2])\n",
    "ttFro, ttL1, ttLinf, ttSpec = [error(tensor.reshape(-1,N), tensor_approx_tt.reshape(-1,N)) for error in normL ]\n",
    "\n",
    "print(\"Frobenius Norm of the Tensor Train Decomposition Error:\\t\\t\", ttFro)\n",
    "print(\"L1 Norm of the Tensor Train Decomposition Error:\\t\\t\", ttL1)\n",
    "print(\"L infinity Norm of the Tensor Train Decomposition Error:\\t\", ttLinf)\n",
    "print(\"Spectral Norm of the Tensor Train Decomposition Error:\\t\\t\", ttSpec)\n",
    "\n",
    "ttError = [ttFro, ttL1, ttLinf, ttSpec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposed Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed tensor @ vec\n",
    "ttTens_Vec = (factors[0].reshape(-1, factors[0].shape[-1])) @ (factors[1]@(factors[2].reshape(-1,factors[2].shape[1]) @ vec))\n",
    "# decomposed tensor @ mat\n",
    "someMatrix.T@factors[0].reshape(-1,factors[0].shape[-1])\n",
    "ttTens_Mat = np.einsum('bc,cbc,cde->d',someMatrix.T@factors[0].reshape(-1,factors[0].shape[-1]), factors[1], factors[2])\n",
    "\n",
    "# decomposed tensor @ someTensor @ decomposed tensor\n",
    "#tuckerTens_tensor_tuckerTens = np.einsum( optimize='optimal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error between decomposed tensor - matrix and tensor-matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.057103619572120914 0.049431848328107646 0.06153015920835039 0.013533434804634734\n",
      "0.0029233901615776763\n"
     ]
    }
   ],
   "source": [
    "ttTensVec_fro, ttTensVec_L1, ttTensVec_Linf, ttTensVec_L2 = [error(tensVec, ttTens_Vec ) for error in normL]\n",
    "\n",
    "print(ttTensVec_fro, ttTensVec_L1, ttTensVec_Linf, ttTensVec_L2)\n",
    "\n",
    "ttTensMat_L2 = err_Spec(tensMat,ttTens_Mat)\n",
    "print(ttTensMat_L2)\n",
    "\n",
    "#parTensTens_fro, parTensTens_L1, parTensTens_Linf, parTensTens_L2 = [error(tensTenstens, parTensTens) for error in normL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "\n",
    "# time of tt tensor x some vector\n",
    "time_ttTens_Vec = timeit.timeit(lambda: (factors[0].reshape(-1, factors[0].shape[-1])) @ (factors[1]@(factors[2].reshape(-1,factors[2].shape[1]) @ vec)), number = n)/n\n",
    "\n",
    "# time of tt tensor x some matrix\n",
    "time_ttTens_Mat = timeit.timeit(lambda: np.einsum('bc,cbc,cde->d',someMatrix.T@factors[0].reshape(-1,factors[0].shape[-1]), factors[1], factors[2], optimize = 'optimal'), number = n)/n\n",
    "\n",
    "# time of tt tensor x some Tensor x tucker tensor\n",
    "#time_ttTens_tensor_ttTens = timeit.timeit( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.648720474146584 6.832922070685422\n"
     ]
    }
   ],
   "source": [
    "speedUP_tensVec_ttTensVec = time_tensVec / time_ttTens_Vec\n",
    "speedUP_tensMat_ttTensMat = time_tensMat / time_ttTens_Mat\n",
    "#speedUP_tensTensortens_ttTensTensorParTens = time_tensTenstens / time_ttTens_tensor_ttTens\n",
    "\n",
    "print(speedUP_tensVec_ttTensVec, speedUP_tensMat_ttTensMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non negative parafac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non negative parafac weight dimension:\t (4,)\n",
      "Non negative parafac factors dimension:\t [312, 32, 128]\n"
     ]
    }
   ],
   "source": [
    "rank_nnp = 4\n",
    "weight, factors = non_negative_parafac(np.abs(tensor), rank_nnp)\n",
    "\n",
    "print(\"Non negative parafac weight dimension:\\t\",weight.shape)\n",
    "print(\"Non negative parafac factors dimension:\\t\", [len(f) for f in factors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NNP Decomposition Time:\t 2.9685137300000006\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "nnpDecTime = timeit.timeit(lambda: non_negative_parafac(np.abs(tensor), rank_nnp), number=n)/n\n",
    "print(\"Average NNP Decomposition Time:\\t\", nnpDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with SVD using 4 Rank:\t 0.9985195062099359\n"
     ]
    }
   ],
   "source": [
    "nnpMemSav = (tensor.nbytes - (weight.nbytes + sum(f.nbytes for f in factors))) / tensor.nbytes\n",
    "print(f\"Memory saving with SVD using {rank_nnp} Rank:\\t\", nnpMemSav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of the NNP Decomposition Error:\t 0.4997873261512946\n",
      "L1 Norm of the NNP Decomposition Error:\t\t 0.5013642050797766\n",
      "L infinity Norm of the NNP Decomposition Error:\t 0.49671763574720673\n",
      "Spectral Norm of the NNP Decomposition Error:\t 0.056523697692167967\n"
     ]
    }
   ],
   "source": [
    "tensor_approx_nnp = np.einsum('il,jl,kl->ijk',(factors[0]*weight),factors[1],factors[2])\n",
    "nnpFro, nnpL1, nnpLinf, nnpSpec = [error(tensor.reshape(-1,N), tensor_approx_nnp.reshape(-1,N)) for error in normL ]\n",
    "\n",
    "print(\"Frobenius Norm of the NNP Decomposition Error:\\t\", nnpFro)\n",
    "print(\"L1 Norm of the NNP Decomposition Error:\\t\\t\", nnpL1)\n",
    "print(\"L infinity Norm of the NNP Decomposition Error:\\t\", nnpLinf)\n",
    "print(\"Spectral Norm of the NNP Decomposition Error:\\t\", nnpSpec)\n",
    "\n",
    "nnpError = [nnpFro, nnpL1,nnpLinf, nnpSpec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposed Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed tensor @ vec\n",
    "nnpTens_Vec = np.einsum('il,jl,l->ij',(factors[0]*weight),factors[1],factors[2].T @ vec)\n",
    "\n",
    "# decomposed tensor @ mat\n",
    "nnpTens_Mat = np.einsum('k,mk,mk,ok->o', weight, someMatrix.T@factors[0], factors[1], factors[2])\n",
    "\n",
    "\n",
    "# decomposed tensor @ someTensor @ decomposed tensor\n",
    "#parTens_tensor_parTens = np.einsum( optimize='optimal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error between decomposed tensor - matrix and tensor-matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05649262660428241 0.049679827705465515 0.06123961807741042 0.01345689392030045\n",
      "0.0029242606258778484\n"
     ]
    }
   ],
   "source": [
    "nnpTensVec_fro, nnpTensVec_L1, nnpTensVec_Linf, nnpTensVec_L2 = [error(tensVec, nnpTens_Vec ) for error in normL]\n",
    "\n",
    "print(nnpTensVec_fro, nnpTensVec_L1, nnpTensVec_Linf, nnpTensVec_L2)\n",
    "\n",
    "nnpTensMat_L2 = err_Spec(tensMat, nnpTens_Mat) \n",
    "\n",
    "print(nnpTensMat_L2)\n",
    "\n",
    "#nnpTensTens_fro, nnpTensTens_L1, nnpTensTens_Linf, nnpTensTens_L2 = [error(tensTenstens, nnpTensTens) for error in normL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "\n",
    "# time of NNP tensor x some vector\n",
    "time_nnpTens_Vec = timeit.timeit(lambda:np.einsum('il,jl,l->ij',(factors[0]*weight),factors[1],factors[2].T @ vec, optimize ='optimal'), number = n)/n\n",
    "\n",
    "# time of NNP tensor x some matrix\n",
    "time_nnpTens_Mat = timeit.timeit(lambda:np.einsum('k,mk,mk,ok->o', weight, someMatrix.T@factors[0], factors[1], factors[2], optimize = 'optimal'), number = n)/n\n",
    "\n",
    "# time of NNP tensor x some Tensor x NNP tensor\n",
    "#time_nnpTens_tensor_nnpTens = timeit.timeit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6365385762641167 4.23370113333564\n"
     ]
    }
   ],
   "source": [
    "speedUP_tensVec_nnpTensVec = time_tensVec / time_nnpTens_Vec\n",
    "speedUP_tensMat_nnpTensMat = time_tensMat / time_nnpTens_Mat\n",
    "#speedUP_tensTensortens_parTensTensorParTens = time_tensTenstens / time_time_parTens_tensor_parTens\n",
    "\n",
    "print(speedUP_tensVec_nnpTensVec,speedUP_tensMat_nnpTensMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clarkson Woodruff Transform (Sketching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg._sketches import cwt_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of the matrix representation of the tensor:\t (9984, 128)\n",
      "The CWT of the matrix representation of the tensor dimension:\t (4, 128)\n"
     ]
    }
   ],
   "source": [
    "print('The dimension of the matrix representation of the tensor:\\t',mat_repr1_tens.shape)\n",
    "sketch_n_rows = 4\n",
    "sketch_mat_repr1_tens = clarkson_woodruff_transform(mat_repr1_tens,sketch_n_rows)\n",
    "sketch_someMatrix = clarkson_woodruff_transform(someMatrix.flatten(),sketch_n_rows)\n",
    "mat_repr1_tens_someMatrix = clarkson_woodruff_transform(np.hstack((someMatrix.reshape(-1,1),mat_repr1_tens)),sketch_n_rows)\n",
    "print('The CWT of the matrix representation of the tensor dimension:\\t', sketch_mat_repr1_tens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sketching Time:\t 0.0013386259999998628\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "sketchDecTime = timeit.timeit((lambda: clarkson_woodruff_transform(mat_repr1_tens,sketch_n_rows)), number=n)/n\n",
    "print(\"Average Sketching Time:\\t\", sketchDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with Sketching using 4 rows:\t 0.9995993589743589\n"
     ]
    }
   ],
   "source": [
    "sketchMemSav = (mat_repr1_tens.nbytes - sketch_mat_repr1_tens.nbytes) / mat_repr1_tens.nbytes\n",
    "print(f\"Memory saving with Sketching using {sketch_n_rows} rows:\\t\", sketchMemSav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clark Woodruf Transformation Error:\t 0.2012423190081187\n"
     ]
    }
   ],
   "source": [
    "decErrCwt_list = []\n",
    "for i in range(0,1000):    \n",
    "    sketch_mat_repr1_tens = clarkson_woodruff_transform(mat_repr1_tens,sketch_n_rows)\n",
    "    decErrCwt = np.abs((norm(mat_repr1_tens) - norm(sketch_mat_repr1_tens))/ norm(mat_repr1_tens))\n",
    "    decErrCwt_list.append(decErrCwt)\n",
    "decErrorCWt = np.mean(decErrCwt_list)\n",
    "print(\"Clark Woodruf Transformation Error:\\t\", decErrorCWt )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error between CWT matrix - vector and Matrix - vector operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of matrix representation of the tensor times a vector:\t (9984,)\n",
      "The Error between CWT of the matrix representation of the tensor times a vector and SVD of the matrix representation of the tensor:\n",
      " 0.13587265595438391\n"
     ]
    }
   ],
   "source": [
    "cwtMatvec_L2 = np.abs(norm(svdMatReprVec)-norm(sketch_mat_repr1_tens@vec))/norm(matRepr1Vec)\n",
    "\n",
    "\n",
    "print('The dimension of matrix representation of the tensor times a vector:\\t', matRepr1Vec.shape)\n",
    "print('The Error between CWT of the matrix representation of the tensor times a vector and SVD of the matrix representation of the tensor:\\n', cwtMatvec_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error between tensor times matrix and the CWT of matrix representation of a tensor times cwt of a matrix:\n",
      " 0.38808593216361204\n",
      "0.38808593216361204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cwt_mat_repr1_tens_someMatrix = clarkson_woodruff_transform(np.hstack((someMatrix.reshape(-1,1),mat_repr1_tens)),sketch_n_rows)\n",
    "\n",
    "\n",
    "mat_repr1_tens_someMatrix = cwt_mat_repr1_tens_someMatrix[:,1:].T @ cwt_mat_repr1_tens_someMatrix[:,0]\n",
    "# The error between tensor times matrix and the CWT of matrix representation of a tensor times cwt of a matrix\n",
    "\n",
    "\n",
    "#cwt1_matRepr1_someMatrix = (norm(tensMat) - norm(mat_repr1_tens_someMatrix)) / norm(tensMat)\n",
    "#cwt2_matRepr1_someMatrix = err_L1(tensMat, mat_repr1_tens_someMatrix )\n",
    "#cwt3_matRepr1_someMatrix = err_Linf(tensMat, mat_repr1_tens_someMatrix )\n",
    "err = []\n",
    "for i in range(0,10000):\n",
    "    err.append(err_Spec(tensMat, mat_repr1_tens_someMatrix ))\n",
    "\n",
    "cwt4_matRepr1_someMatrix = np.mean(err)\n",
    "print('The error between tensor times matrix and the CWT of matrix representation of a tensor times cwt of a matrix:\\n',cwt4_matRepr1_someMatrix)\n",
    "\n",
    "print(cwt4_matRepr1_someMatrix )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of W:\t (9984, 4)\n",
      "Dimension of H:\t (4, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 5000 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_rank = 4\n",
    "model = NMF(nmf_rank, init= 'random', max_iter = 5000)\n",
    "W = model.fit_transform(np.abs(mat_repr1_tens))\n",
    "H = model.components_\n",
    "\n",
    "print('Dimension of W:\\t',W.shape)\n",
    "print('Dimension of H:\\t',H.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeNMF(model):\n",
    "    model.fit_transform(np.abs(mat_repr1_tens))\n",
    "    model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NMF Decomposition Time:\t 9.078264899999965\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "nmfDecTime = timeit.timeit(lambda: timeNMF(model), number=n)/n\n",
    "print(\"Average NMF Decomposition Time:\\t\", nmfDecTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saving with NMF using 4 Rank:\t 0.9683493589743589\n"
     ]
    }
   ],
   "source": [
    "memSavNMF = (tensor.nbytes - sum([W.nbytes, H.nbytes])) / tensor.nbytes\n",
    "print(f\"Memory saving with NMF using {nmf_rank} Rank:\\t\", memSavNMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of the SVD Decomposition Error:\t 0.4908784564716223\n",
      "L1 Norm of the SVD Decomposition Error:\t\t 0.4952926859195579\n",
      "L infinity Norm of the SVD Decomposition Error:\t 0.4858901595169547\n",
      "Spectral Norm of the SVD Decomposition Error:\t 0.05590762175870665\n"
     ]
    }
   ],
   "source": [
    "mat_repr1_recons_NMF = W@H\n",
    "nmfFro, nmfL1, nmfLinf, nmfSpec = [error(mat_repr1_tens, mat_repr1_recons_NMF) for error in normL ]\n",
    "\n",
    "print(\"Frobenius Norm of the SVD Decomposition Error:\\t\", nmfFro)\n",
    "print(\"L1 Norm of the SVD Decomposition Error:\\t\\t\", nmfL1)\n",
    "print(\"L infinity Norm of the SVD Decomposition Error:\\t\", nmfLinf)\n",
    "print(\"Spectral Norm of the SVD Decomposition Error:\\t\", nmfSpec)\n",
    "\n",
    "nmfError = [nmfFro,nmfL1,nmfLinf,nmfSpec ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error in decomposed matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028540476364002802 0.024541645431512087 0.03094416625480781 0.006575504574033218\n",
      "0.00287804985136155\n",
      "0.0014313821437046054 0.002163736335759442 0.0019109609598716654 0.0006712705665006264\n"
     ]
    }
   ],
   "source": [
    "# nmf of matrix representation of tensor x some vector\n",
    "nmfMatReprVec = W@(H@vec)\n",
    "\n",
    "# nmf of matrix representation of tensor x some matrix\n",
    "nmfMatReprMat = (someMatrix.flatten().T @ W)@H\n",
    "\n",
    "# nmf of matrix representation of tensor x some Tensor x nmf of matrix representation of tensor \n",
    "nmfMatReprTensNmfMatRepr = H.T @ ( W.T @ bsr @ W) @ H\n",
    "\n",
    "nmfDecVec_fro, nmfDecVec_L1, nmfDecVec_Linf, nmfDecVec_L2 = [norm(tensVec,nmfMatReprVec.reshape(tensVec.shape)) for norm in normL]\n",
    "print(nmfDecVec_fro, nmfDecVec_L1, nmfDecVec_Linf, nmfDecVec_L2)\n",
    "\n",
    "tensMat_nmfMatReprMat_L2 = err_Spec(tensMat,nmfMatReprMat)\n",
    "print(tensMat_nmfMatReprMat_L2)\n",
    "\n",
    "nmfMatReprTensNmfMatRepr_tensTenstens_fro, nmfMatReprTensNmfMatRepr_tensTenstens_L1, nmfMatReprTensNmfMatRepr_tensTenstens_Linf, nmfMatReprTensNmfMatRepr_tensTenstens_L2 = [norm(tensTenstens,nmfMatReprTensNmfMatRepr) for norm in normL]\n",
    "print(nmfMatReprTensNmfMatRepr_tensTenstens_fro, nmfMatReprTensNmfMatRepr_tensTenstens_L1, nmfMatReprTensNmfMatRepr_tensTenstens_Linf, nmfMatReprTensNmfMatRepr_tensTenstens_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Speed Up Measures in Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1814049053740645 1.376289567136838 1.6259552458306084\n",
      "1.2453500981083456 3.7434812372780852 4.661944726111015\n",
      "1.9019021034218673 9.407355644215414 17.891869487370872\n"
     ]
    }
   ],
   "source": [
    "# time of nmf of matrix representation of tensor x some vector\n",
    "time_nmfMatReprVec = timeit.timeit( lambda: W@(H@vec), number = n)/n\n",
    "\n",
    "# time of nmf of matrix representation of tensor x some matrix\n",
    "time_nmfMatReprMat = timeit.timeit(lambda: flattedMat.T @ W@H, number = n)/n\n",
    "\n",
    "# time of nmf of matrix representation of tensor x some Tensor x nmf of matrix representation of tensor\n",
    "time_nmfMatReprTensNmfMatRepr = timeit.timeit( lambda:H.T @ ( W.T @ bsr @ W) @ H, number = n)/n\n",
    "\n",
    "# Speed up in operations\n",
    "\n",
    "### \n",
    "speedup_tensVec_matReprVec = time_tensVec / time_matRepr1Vec\n",
    "speedup_matRepr1Vec_nmfMatReprVec =  time_matRepr1Vec / time_nmfMatReprVec\n",
    "speedup_tensVec_nmfMatReprVec = time_tensVec / time_nmfMatReprVec\n",
    "\n",
    "###\n",
    "speedup_tensMat_matRepr1Mat = time_tensMat / time_matRepr1Mat\n",
    "speedup_matRepr1Mat_nmfMatReprMat = time_matRepr1Mat / time_nmfMatReprMat\n",
    "speedup_tensMat_nmfMatReprMat = time_tensMat / time_nmfMatReprMat\n",
    "\n",
    "###\n",
    "speedup_tenstenstens_matReprTensMatRepr = time_tensTenstens / time_matReprTensMatRepr\n",
    "speedup_matReprTensMatRepr_nmfMatReprTensNmfMatRepr = time_matReprTensMatRepr / time_nmfMatReprTensNmfMatRepr \n",
    "speedup_tenstenstens_nmfMatReprTensNmfMatRepr = time_tensTenstens / time_nmfMatReprTensNmfMatRepr \n",
    "\n",
    "print(speedup_tensVec_matReprVec, speedup_matRepr1Vec_nmfMatReprVec, speedup_tensVec_nmfMatReprVec)\n",
    "print(speedup_tensMat_matRepr1Mat, speedup_matRepr1Mat_nmfMatReprMat, speedup_tensMat_nmfMatReprMat)\n",
    "print(speedup_tenstenstens_matReprTensMatRepr, speedup_matReprTensMatRepr_nmfMatReprTensNmfMatRepr, speedup_tenstenstens_nmfMatReprTensNmfMatRepr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Bar plot for error, time, memory saving (among the decomposition types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "nameAxis = ['SVD '+f'\\n(Rank: {svd_rank})', 'Tucker '+f'\\n(Rank: {rank_tucker})', 'Parafac '+f'\\n(Rank: {rank_parafac})', 'Tensor Train '+f'\\n(Rank: {rank_tt})', 'Non-negative Parafac '+f'\\n(Rank: {rank_nnp})', 'NMF '+f'\\n(Rank: {nmf_rank})']\n",
    "decErrAxis = [100*svdFro, 100*tuckerFro, 100*parFro, 100*ttFro, 100*nnpFro, 100*nmfFro]\n",
    "decTimeAxis = [svdDecTime, tuckerDecTime, parDecTime, ttDecTime, nnpDecTime, nmfDecTime ]\n",
    "memSavAxis = [100*memSavSvd, 100*memSavTucker, 100*parMemSav, 100*memSavTT, 100*nnpMemSav, 100*memSavNMF ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"deepskyblue\", \"yellowgreen\"]\n",
    "y_axis = [decTimeAxis, memSavAxis ]\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20,16), sharex=True)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].bar(nameAxis, y_axis[i], color=colors[i], width = .75)\n",
    "    ax[i].yaxis.grid(color='b', alpha=0.5, linestyle='dashed', linewidth=0.5)\n",
    "    ax[i].set_axisbelow(True)\n",
    "    ax[i].set_ylim(min(y_axis[i])- np.mean(y_axis[i])*0.025, max(y_axis[i])+ np.mean(y_axis[i])*0.05)\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=16)\n",
    "    for h, v in enumerate(y_axis[i]):\n",
    "        ax[i].text(h-.20, v+.0025, str(np.round(v,6)), color='black', size = 16, weight = \"bold\")\n",
    "\n",
    "plt.xlabel(\"Decomposition Types\", fontsize=24)\n",
    "\n",
    "ax[0].set_ylabel(\"Dec. Time [sec]\", fontsize=18)\n",
    "ax[1].set_ylabel(\"Rel. Memory Saving [-]\", fontsize=18)\n",
    "ax[0].yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax[1].yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax[1].set_ylim(min(y_axis[i])- np.mean(y_axis[i])*0.025, 100)\n",
    "plt.savefig('fig/SavTimeError.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "uniSlightBlue = '#00BEFF'\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20,8))\n",
    "ax.bar(operList, tensorOperationTimes, color = uniSlightBlue)\n",
    "ax.yaxis.grid(color='b', alpha=0.5, linestyle='dashed', linewidth=0.5)\n",
    "plt.xticks(rotation=22.5)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_ylabel(\"Elapsed Time [sec]\", fontsize=24)\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True) \n",
    "formatter.set_powerlimits((-1,1)) \n",
    "ax.yaxis.set_major_formatter(formatter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO #1\n",
    "# Speed comparion is not implemented. No function definition is defined for decomposed_tensor @ Vec multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO #2\n",
    "# For every decomposition type -> Line Plot: operation error, speedup time in tensor operations vs decomposed tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE! (11.07.21)\n",
    "# For every decomposition type -> Bar plot: Decomposition error, decomposition time, memory saving"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "tensor_decomposition_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
